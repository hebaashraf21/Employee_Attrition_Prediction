{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade seaborn matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() \n",
    "relative_path = os.path.join('..', '..','data', 'train.csv')\n",
    "train_data = pd.read_csv(os.path.join(current_dir, relative_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join('..', '..','data', 'test.csv')\n",
    "test_data = pd.read_csv(os.path.join(current_dir, relative_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.drop([\"Attrition\"], axis = 1)\n",
    "y_train = train_data[\"Attrition\"]\n",
    "\n",
    "x_test = test_data.drop([\"Attrition\"], axis = 1)\n",
    "y_test = test_data[\"Attrition\"]\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of negative samples to positive samples\n",
    "ratio = len(y_train[y_train == 0]) / len(y_train[y_train == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying different Learning Rates (eta)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate : 0.01\n",
      "Average Accuracy: 0.7659770114942529\n",
      "Average F1 Score: 0.17444444444444446\n",
      "Average Precision: 0.15178571428571427\n",
      "Average Recall: 0.22333333333333333\n",
      "Average Training Time (seconds): 0.3886226177215576\n",
      "-----------------------------------------------------\n",
      "For learning rate : 0.05\n",
      "Average Accuracy: 0.8434482758620689\n",
      "Average F1 Score: 0.21746031746031744\n",
      "Average Precision: 0.22333333333333333\n",
      "Average Recall: 0.21833333333333332\n",
      "Average Training Time (seconds): 0.22903568744659425\n",
      "-----------------------------------------------------\n",
      "For learning rate : 0.1\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.19183757305145263\n",
      "-----------------------------------------------------\n",
      "For learning rate : 0.3\n",
      "Average Accuracy: 0.8331034482758621\n",
      "Average F1 Score: 0.19025252525252526\n",
      "Average Precision: 0.19357142857142856\n",
      "Average Recall: 0.19833333333333333\n",
      "Average Training Time (seconds): 0.17582488059997559\n",
      "-----------------------------------------------------\n",
      "For learning rate : 0.5\n",
      "Average Accuracy: 0.8400000000000001\n",
      "Average F1 Score: 0.2065873015873016\n",
      "Average Precision: 0.225\n",
      "Average Recall: 0.19833333333333333\n",
      "Average Training Time (seconds): 0.19664812088012695\n",
      "-----------------------------------------------------\n",
      "For learning rate : 0.7\n",
      "Average Accuracy: 0.8058620689655174\n",
      "Average F1 Score: 0.14222222222222222\n",
      "Average Precision: 0.13333333333333333\n",
      "Average Recall: 0.15333333333333332\n",
      "Average Training Time (seconds): 0.15650758743286133\n",
      "-----------------------------------------------------\n",
      "For learning rate : 0.9\n",
      "Average Accuracy: 0.8297701149425288\n",
      "Average F1 Score: 0.2172222222222222\n",
      "Average Precision: 0.265\n",
      "Average Recall: 0.20666666666666664\n",
      "Average Training Time (seconds): 0.2096395492553711\n",
      "-----------------------------------------------------\n",
      "For learning rate : 1\n",
      "Average Accuracy: 0.8164367816091953\n",
      "Average F1 Score: 0.16936507936507939\n",
      "Average Precision: 0.1716666666666667\n",
      "Average Recall: 0.17333333333333334\n",
      "Average Training Time (seconds): 0.15302226543426514\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "for lr in learning_rates:\n",
    "    params = {\n",
    "        'eta': lr,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': 6,               # Maximum depth of a tree\n",
    "        'gamma': 0.1,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': 100,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': 1,                  # L2 regularization term on weights\n",
    "        'alpha': 0,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For learning rate :\", lr)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different Number Of Trees (num_boost_round)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Number of trees : 50\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.6980184316635132\n",
      "-----------------------------------------------------\n",
      "For Number of trees : 100\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.24061594009399415\n",
      "-----------------------------------------------------\n",
      "For Number of trees : 200\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.30039396286010744\n",
      "-----------------------------------------------------\n",
      "For Number of trees : 300\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.34595048427581787\n",
      "-----------------------------------------------------\n",
      "For Number of trees : 500\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.6199766635894776\n",
      "-----------------------------------------------------\n",
      "For Number of trees : 800\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.4753666639328003\n",
      "-----------------------------------------------------\n",
      "For Number of trees : 1000\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.40387601852416993\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_boost_round_list = [50, 100, 200, 300, 500, 800, 1000]\n",
    "for num_boost_round in num_boost_round_list:\n",
    "    params = {\n",
    "        'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': 6,               # Maximum depth of a tree\n",
    "        'gamma': 0.1,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': 1,                  # L2 regularization term on weights\n",
    "        'alpha': 0,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For Number of trees :\", num_boost_round)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different Trees Depth (max_depth)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth: 2\n",
      "Average Accuracy: 0.7897701149425288\n",
      "Average F1 Score: 0.21207070707070708\n",
      "Average Precision: 0.2542857142857143\n",
      "Average Recall: 0.22333333333333333\n",
      "Average Training Time (seconds): 0.19703178405761718\n",
      "-----------------------------------------------------\n",
      "For max_depth: 4\n",
      "Average Accuracy: 0.8267816091954023\n",
      "Average F1 Score: 0.1773809523809524\n",
      "Average Precision: 0.18666666666666665\n",
      "Average Recall: 0.17333333333333334\n",
      "Average Training Time (seconds): 0.31042745113372805\n",
      "-----------------------------------------------------\n",
      "For max_depth: 6\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.20820105075836182\n",
      "-----------------------------------------------------\n",
      "For max_depth: 8\n",
      "Average Accuracy: 0.8502298850574712\n",
      "Average F1 Score: 0.21904761904761907\n",
      "Average Precision: 0.22666666666666666\n",
      "Average Recall: 0.21833333333333332\n",
      "Average Training Time (seconds): 0.22655913829803467\n",
      "-----------------------------------------------------\n",
      "For max_depth: 10\n",
      "Average Accuracy: 0.8468965517241379\n",
      "Average F1 Score: 0.20047619047619047\n",
      "Average Precision: 0.21000000000000002\n",
      "Average Recall: 0.1933333333333333\n",
      "Average Training Time (seconds): 0.23881161212921143\n",
      "-----------------------------------------------------\n",
      "For max_depth: 12\n",
      "Average Accuracy: 0.8434482758620689\n",
      "Average F1 Score: 0.1976984126984127\n",
      "Average Precision: 0.20500000000000002\n",
      "Average Recall: 0.1933333333333333\n",
      "Average Training Time (seconds): 0.27712669372558596\n",
      "-----------------------------------------------------\n",
      "For max_depth: 14\n",
      "Average Accuracy: 0.8434482758620689\n",
      "Average F1 Score: 0.1976984126984127\n",
      "Average Precision: 0.20500000000000002\n",
      "Average Recall: 0.1933333333333333\n",
      "Average Training Time (seconds): 0.27740814685821535\n",
      "-----------------------------------------------------\n",
      "For max_depth: 16\n",
      "Average Accuracy: 0.8434482758620689\n",
      "Average F1 Score: 0.1976984126984127\n",
      "Average Precision: 0.20500000000000002\n",
      "Average Recall: 0.1933333333333333\n",
      "Average Training Time (seconds): 0.2510829448699951\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_depth_list = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "for max_depth in max_depth_list:\n",
    "    params = {\n",
    "        'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': max_depth,               # Maximum depth of a tree\n",
    "        'gamma': 0.1,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': 100,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': 1,                  # L2 regularization term on weights\n",
    "        'alpha': 0,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For max_depth:\", max_depth)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different combinations of max_depth & num_boost_round**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.23621933621933625\n",
      "Corresponding Accuracy: 0.8504597701149426\n",
      "Corresponding num_boost_round: 1000\n",
      "Corresponding max_depth: 6\n"
     ]
    }
   ],
   "source": [
    "num_boost_round_list = [50, 100, 200, 300, 500, 800, 1000]\n",
    "max_depth_list = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "max_f1_score = 0\n",
    "best_accuracy = 0\n",
    "best_max_depth = None\n",
    "best_num_boost_round = None\n",
    "\n",
    "for num_boost_round in num_boost_round_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        params = {\n",
    "            'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "            'max_depth': max_depth,               # Maximum depth of a tree\n",
    "            'gamma': 0.1,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "            'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "            'num_boost_round': num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "            'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "            'lambda': 1,                  # L2 regularization term on weights\n",
    "            'alpha': 0,                   # L1 regularization term on weights\n",
    "            \n",
    "            'eval_metric': 'error',           # Evaluation metric used during training\n",
    "            'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "            'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "            'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "            'verbosity': 0,               # Verbosity of output messages\n",
    "        }\n",
    "\n",
    "\n",
    "        # Initialize CatBoost classifier\n",
    "        xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "        # Train the model using KFold cross-validation\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        training_times = []\n",
    "\n",
    "        for train_index, val_index in k_fold.split(x_train):\n",
    "            X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "            \n",
    "            # Record start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Fit the model\n",
    "            xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # Record end time\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate training time\n",
    "            training_time = end_time - start_time\n",
    "            training_times.append(training_time)\n",
    "            \n",
    "            # Predict on validation set\n",
    "            y_pred = xgb_model.predict(X_val_fold)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "            precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "            recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "            f1 = f1_score(y_val_fold, y_pred)\n",
    "            \n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "\n",
    "        # Calculate and print average metrics\n",
    "        avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "        avg_precision = sum(precisions) / len(precisions)\n",
    "        avg_recall = sum(recalls) / len(recalls)\n",
    "        avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "        if avg_f1_score > max_f1_score:\n",
    "            max_f1_score = avg_f1_score\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_num_boost_round = num_boost_round\n",
    "            best_max_depth = max_depth\n",
    "\n",
    "# Print the results for the best F1 score\n",
    "print(\"Best F1 Score:\", max_f1_score)\n",
    "print(\"Corresponding Accuracy:\", best_accuracy)\n",
    "print(\"Corresponding num_boost_round:\", num_boost_round)\n",
    "print(\"Corresponding max_depth:\", best_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different gamma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For gamma: 0.001\n",
      "Average Accuracy: 0.8470114942528737\n",
      "Average F1 Score: 0.23185425685425684\n",
      "Average Precision: 0.22833333333333333\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.36702957153320315\n",
      "-----------------------------------------------------\n",
      "For gamma: 0.01\n",
      "Average Accuracy: 0.8470114942528735\n",
      "Average F1 Score: 0.2219047619047619\n",
      "Average Precision: 0.22666666666666666\n",
      "Average Recall: 0.21833333333333332\n",
      "Average Training Time (seconds): 0.39857404232025145\n",
      "-----------------------------------------------------\n",
      "For gamma: 0.1\n",
      "Average Accuracy: 0.8504597701149426\n",
      "Average F1 Score: 0.23621933621933625\n",
      "Average Precision: 0.24\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 1.7477010488510132\n",
      "-----------------------------------------------------\n",
      "For gamma: 1\n",
      "Average Accuracy: 0.8299999999999998\n",
      "Average F1 Score: 0.21833333333333332\n",
      "Average Precision: 0.20166666666666666\n",
      "Average Recall: 0.24333333333333332\n",
      "Average Training Time (seconds): 0.22058136463165284\n",
      "-----------------------------------------------------\n",
      "For gamma: 5\n",
      "Average Accuracy: 0.782528735632184\n",
      "Average F1 Score: 0.2363869463869464\n",
      "Average Precision: 0.18845238095238095\n",
      "Average Recall: 0.32166666666666666\n",
      "Average Training Time (seconds): 0.09247918128967285\n",
      "-----------------------------------------------------\n",
      "For gamma: 10\n",
      "Average Accuracy: 0.7382758620689656\n",
      "Average F1 Score: 0.20477633477633478\n",
      "Average Precision: 0.1569047619047619\n",
      "Average Recall: 0.30166666666666664\n",
      "Average Training Time (seconds): 0.07915689945220947\n",
      "-----------------------------------------------------\n",
      "For gamma: 0\n",
      "Average Accuracy: 0.8470114942528737\n",
      "Average F1 Score: 0.23185425685425684\n",
      "Average Precision: 0.22833333333333333\n",
      "Average Recall: 0.23833333333333334\n",
      "Average Training Time (seconds): 0.14405350685119628\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gamma_list = [0.001, 0.01, 0.1, 1, 5, 10, 0]\n",
    "for gamma in gamma_list:\n",
    "    params = {\n",
    "        'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': 6,               # Maximum depth of a tree\n",
    "        'gamma': gamma,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': 100,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': 1,                  # L2 regularization term on weights\n",
    "        'alpha': 0,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For gamma:\", gamma)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different values for lambda regularization parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For lambda: 0.001\n",
      "Average Accuracy: 0.7788505747126437\n",
      "Average F1 Score: 0.1808080808080808\n",
      "Average Precision: 0.15583333333333332\n",
      "Average Recall: 0.22333333333333333\n",
      "Average Training Time (seconds): 0.375748348236084\n",
      "-----------------------------------------------------\n",
      "For lambda: 0.01\n",
      "Average Accuracy: 0.772183908045977\n",
      "Average F1 Score: 0.1758080808080808\n",
      "Average Precision: 0.15011904761904762\n",
      "Average Recall: 0.22333333333333333\n",
      "Average Training Time (seconds): 0.22609076499938965\n",
      "-----------------------------------------------------\n",
      "For lambda: 0.1\n",
      "Average Accuracy: 0.7857471264367817\n",
      "Average F1 Score: 0.19166666666666668\n",
      "Average Precision: 0.16202380952380951\n",
      "Average Recall: 0.24333333333333332\n",
      "Average Training Time (seconds): 0.35608491897583006\n",
      "-----------------------------------------------------\n",
      "For lambda: 1\n",
      "Average Accuracy: 0.782528735632184\n",
      "Average F1 Score: 0.2363869463869464\n",
      "Average Precision: 0.18845238095238095\n",
      "Average Recall: 0.32166666666666666\n",
      "Average Training Time (seconds): 0.28807332515716555\n",
      "-----------------------------------------------------\n",
      "For lambda: 10\n",
      "Average Accuracy: 0.7283908045977011\n",
      "Average F1 Score: 0.1931068931068931\n",
      "Average Precision: 0.1445238095238095\n",
      "Average Recall: 0.2966666666666667\n",
      "Average Training Time (seconds): 0.19108893871307372\n",
      "-----------------------------------------------------\n",
      "For lambda: 100\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.19466042518615723\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lambda_list = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for lambda_l2 in lambda_list:\n",
    "    params = {\n",
    "        'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': 6,               # Maximum depth of a tree\n",
    "        'gamma': 5,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': 100,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': lambda_l2,                  # L2 regularization term on weights\n",
    "        'alpha': 0,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For lambda:\", lambda_l2)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different values for alpha regularization parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For alpha: 0\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.2508460760116577\n",
      "-----------------------------------------------------\n",
      "For alpha: 0.001\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.20649917125701905\n",
      "-----------------------------------------------------\n",
      "For alpha: 0.01\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.20129239559173584\n",
      "-----------------------------------------------------\n",
      "For alpha: 0.1\n",
      "Average Accuracy: 0.6401149425287356\n",
      "Average F1 Score: 0.2620928778823516\n",
      "Average Precision: 0.17432400932400932\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.1516879081726074\n",
      "-----------------------------------------------------\n",
      "For alpha: 1\n",
      "Average Accuracy: 0.6367816091954023\n",
      "Average F1 Score: 0.26062228964705747\n",
      "Average Precision: 0.17322510822510823\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.19130330085754393\n",
      "-----------------------------------------------------\n",
      "For alpha: 10\n",
      "Average Accuracy: 0.575287356321839\n",
      "Average F1 Score: 0.22573475544063779\n",
      "Average Precision: 0.1466804029304029\n",
      "Average Recall: 0.5849999999999999\n",
      "Average Training Time (seconds): 0.1780909299850464\n",
      "-----------------------------------------------------\n",
      "For alpha: 100\n",
      "Average Accuracy: 0.534712643678161\n",
      "Average F1 Score: 0.06178030303030303\n",
      "Average Precision: 0.033793103448275866\n",
      "Average Recall: 0.4\n",
      "Average Training Time (seconds): 0.11535129547119141\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for alpha in alpha_list:\n",
    "    params = {\n",
    "        'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': 6,               # Maximum depth of a tree\n",
    "        'gamma': 5,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': 1,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': 100,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': 100,                  # L2 regularization term on weights\n",
    "        'alpha': alpha,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For alpha:\", alpha)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different values for min_child_weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For min_child_weight: 0.1\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.2711900234222412\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 0.5\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.22164499759674072\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 1\n",
      "Average Accuracy: 0.6435632183908047\n",
      "Average F1 Score: 0.26531215860163226\n",
      "Average Precision: 0.17679875679875678\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.2967196226119995\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 5\n",
      "Average Accuracy: 0.6401149425287356\n",
      "Average F1 Score: 0.26281215860163226\n",
      "Average Precision: 0.17452602952602952\n",
      "Average Recall: 0.6016666666666666\n",
      "Average Training Time (seconds): 0.1824885606765747\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 10\n",
      "Average Accuracy: 0.6501149425287357\n",
      "Average F1 Score: 0.27528104248692487\n",
      "Average Precision: 0.18284715284715283\n",
      "Average Recall: 0.6266666666666666\n",
      "Average Training Time (seconds): 0.1448887825012207\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 20\n",
      "Average Accuracy: 0.6537931034482758\n",
      "Average F1 Score: 0.2800221708116445\n",
      "Average Precision: 0.18566100566100566\n",
      "Average Recall: 0.6216666666666666\n",
      "Average Training Time (seconds): 0.14139163494110107\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 40\n",
      "Average Accuracy: 0.6739080459770115\n",
      "Average F1 Score: 0.29476846682729035\n",
      "Average Precision: 0.20071844821844823\n",
      "Average Recall: 0.605\n",
      "Average Training Time (seconds): 0.12032010555267333\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 60\n",
      "Average Accuracy: 0.534712643678161\n",
      "Average F1 Score: 0.06178030303030303\n",
      "Average Precision: 0.033793103448275866\n",
      "Average Recall: 0.4\n",
      "Average Training Time (seconds): 0.08567874431610108\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 80\n",
      "Average Accuracy: 0.534712643678161\n",
      "Average F1 Score: 0.06178030303030303\n",
      "Average Precision: 0.033793103448275866\n",
      "Average Recall: 0.4\n",
      "Average Training Time (seconds): 0.15133171081542968\n",
      "-----------------------------------------------------\n",
      "For min_child_weight: 100\n",
      "Average Accuracy: 0.534712643678161\n",
      "Average F1 Score: 0.06178030303030303\n",
      "Average Precision: 0.033793103448275866\n",
      "Average Recall: 0.4\n",
      "Average Training Time (seconds): 0.12304213047027587\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "min_child_weight_list = [0.1, 0.5, 1, 5, 10, 20, 40, 60, 80, 100]\n",
    "for min_child_weight in min_child_weight_list:\n",
    "    params = {\n",
    "        'eta': 0.1,                   # Learning rate (step size shrinkage)\n",
    "        'max_depth': 6,               # Maximum depth of a tree\n",
    "        'gamma': 5,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "        'min_child_weight': min_child_weight,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "        'num_boost_round': 100,       # Number of boosting rounds (trees) to run\n",
    "        'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "        'lambda': 100,                  # L2 regularization term on weights\n",
    "        'alpha': 0,                   # L1 regularization term on weights\n",
    "        \n",
    "        'eval_metric': 'error',           # Evaluation metric used during training\n",
    "        'booster': 'gbtree',          # Type of boosting model\n",
    "\n",
    "        'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "        'objective': 'binary:logistic',  # Learning task and corresponding objective function\n",
    "        'verbosity': 0,               # Verbosity of output messages\n",
    "    }\n",
    "\n",
    "\n",
    "    # Initialize CatBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "    # Train the model using KFold cross-validation\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for train_index, val_index in k_fold.split(x_train):\n",
    "        X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Fit the model\n",
    "        xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate training time\n",
    "        training_time = end_time - start_time\n",
    "        training_times.append(training_time)\n",
    "        \n",
    "        # Predict on validation set\n",
    "        y_pred = xgb_model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "        f1 = f1_score(y_val_fold, y_pred)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    # Calculate and print average metrics\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "    # Calculate average training time\n",
    "    avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "    print(\"For min_child_weight:\", min_child_weight)\n",
    "    print(\"Average Accuracy:\", avg_accuracy)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)\n",
    "    print(\"Average Precision:\", avg_precision)\n",
    "    print(\"Average Recall:\", avg_recall)\n",
    "    print(\"Average Training Time (seconds):\", avg_training_time)\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Different Combinations Of Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "num_boost_round_list = [50, 100, 200, 300, 500, 800, 1000]\n",
    "max_depth_list = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "gamma_list = [0.001, 0.01, 0.1, 1, 5, 10, 0]\n",
    "lambda_list = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "alpha_list = [0, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "min_child_weight_list = [0.1, 0.5, 1, 5, 10, 20, 40, 60, 80, 100]\n",
    "eval_metric_list = ['error', 'logloss', 'auc', 'aucpr']\n",
    "objective_list = ['binary:logistic', 'binary:logitraw', 'binary:hinge']\n",
    "booster_list = ['gbtree', 'gblinear', 'dart']\n",
    "\n",
    "max_f1_score = 0\n",
    "best_accuracy = 0\n",
    "best_accuracy = None\n",
    "best_num_boost_round = None\n",
    "best_max_depth = None\n",
    "best_lr = None\n",
    "best_gamma = None\n",
    "best_lambda = None\n",
    "best_alpha = None\n",
    "best_min_child_weight = None\n",
    "best_eval_metric = None\n",
    "best_objective = None\n",
    "best_booster = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for num_boost_round in num_boost_round_list:\n",
    "        for max_depth in max_depth_list:\n",
    "            for gamma in gamma_list:\n",
    "                for lambda_l2 in lambda_list:\n",
    "                    for alpha in alpha_list:\n",
    "                        for min_child_weight in min_child_weight_list:\n",
    "                            for eval_metric in eval_metric_list:\n",
    "                                for objective in objective_list:\n",
    "                                    for booster in booster_list:\n",
    "                                        params = {\n",
    "                                            'eta': lr,                   # Learning rate (step size shrinkage)\n",
    "                                            'max_depth': max_depth,               # Maximum depth of a tree\n",
    "                                            'gamma': gamma,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "                                            'min_child_weight': min_child_weight,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "                                            'num_boost_round': num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "                                            'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "                                            'lambda': lambda_l2,                  # L2 regularization term on weights\n",
    "                                            'alpha': alpha,                   # L1 regularization term on weights\n",
    "                                            \n",
    "                                            'eval_metric': eval_metric,           # Evaluation metric used during training\n",
    "                                            'booster': booster,          # Type of boosting model\n",
    "\n",
    "                                            'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "                                            'objective': objective,  # Learning task and corresponding objective function\n",
    "                                            'verbosity': 0,               # Verbosity of output messages\n",
    "                                        }\n",
    "\n",
    "\n",
    "                                        # Initialize CatBoost classifier\n",
    "                                        xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "                                        # Train the model using KFold cross-validation\n",
    "                                        accuracies = []\n",
    "                                        precisions = []\n",
    "                                        recalls = []\n",
    "                                        f1_scores = []\n",
    "                                        training_times = []\n",
    "\n",
    "                                        for train_index, val_index in k_fold.split(x_train):\n",
    "                                            X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "                                            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                                            \n",
    "                                            # Record start time\n",
    "                                            start_time = time.time()\n",
    "\n",
    "                                            # Fit the model\n",
    "                                            xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                                            # Record end time\n",
    "                                            end_time = time.time()\n",
    "\n",
    "                                            # Calculate training time\n",
    "                                            training_time = end_time - start_time\n",
    "                                            training_times.append(training_time)\n",
    "                                            \n",
    "                                            # Predict on validation set\n",
    "                                            y_pred = xgb_model.predict(X_val_fold)\n",
    "                                            \n",
    "                                            # Calculate metrics\n",
    "                                            accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "                                            precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "                                            recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "                                            f1 = f1_score(y_val_fold, y_pred)\n",
    "                                            \n",
    "                                            accuracies.append(accuracy)\n",
    "                                            precisions.append(precision)\n",
    "                                            recalls.append(recall)\n",
    "                                            f1_scores.append(f1)\n",
    "\n",
    "\n",
    "                                        # Calculate and print average metrics\n",
    "                                        avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "                                        avg_precision = sum(precisions) / len(precisions)\n",
    "                                        avg_recall = sum(recalls) / len(recalls)\n",
    "                                        avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "                                        if avg_f1_score > max_f1_score:\n",
    "                                            max_f1_score = avg_f1_score\n",
    "                                            best_accuracy = avg_accuracy\n",
    "                                            best_num_boost_round = num_boost_round\n",
    "                                            best_max_depth = max_depth\n",
    "                                            best_lr = lr\n",
    "                                            best_gamma = gamma\n",
    "                                            best_lambda = lambda_l2\n",
    "                                            best_alpha = alpha\n",
    "                                            best_min_child_weight = min_child_weight\n",
    "                                            best_eval_metric = eval_metric\n",
    "                                            best_objective = objective\n",
    "                                            best_booster = booster\n",
    "\n",
    "\n",
    "# Print the results for the best F1 score\n",
    "print(\"Best F1 Score:\", max_f1_score)\n",
    "print(\"Corresponding Accuracy:\", best_accuracy)\n",
    "\n",
    "print(\"Corresponding num_boost_round:\", best_num_boost_round)\n",
    "print(\"Corresponding max_depth:\", best_max_depth)\n",
    "print(\"Corresponding learning rate:\", best_lr)\n",
    "print(\"Corresponding gamma:\", best_gamma)\n",
    "print(\"Corresponding lambda:\", best_lambda)\n",
    "print(\"Corresponding alpha:\", best_alpha)\n",
    "print(\"Corresponding min_child_weight:\", best_min_child_weight)\n",
    "print(\"Corresponding eval_metric:\", best_eval_metric)\n",
    "print(\"Corresponding objective:\", best_objective)\n",
    "print(\"Corresponding booster:\", best_booster)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the model on the train set using the best used parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': best_lr,                   # Learning rate (step size shrinkage)\n",
    "    'max_depth': best_max_depth,               # Maximum depth of a tree\n",
    "    'gamma': best_gamma,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'min_child_weight': best_min_child_weight,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "    'num_boost_round': best_num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "    'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "    'lambda': best_lambda,                  # L2 regularization term on weights\n",
    "    'alpha': best_alpha,                   # L1 regularization term on weights\n",
    "    \n",
    "    'eval_metric': best_eval_metric,           # Evaluation metric used during training\n",
    "    'booster': best_booster,          # Type of boosting model\n",
    "\n",
    "    'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "    'objective': best_objective,  # Learning task and corresponding objective function\n",
    "    'verbosity': 0,               # Verbosity of output messages\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Train the model using KFold cross-validation\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "training_times = []\n",
    "\n",
    "for train_index, val_index in k_fold.split(x_train):\n",
    "    X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = xgb_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    f1 = f1_score(y_val_fold, y_pred)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Calculate average training time\n",
    "avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "print(\"Validation Average Accuracy:\", avg_accuracy)\n",
    "print(\"Validation Average F1 Score:\", avg_f1_score)\n",
    "print(\"Validation Average Precision:\", avg_precision)\n",
    "print(\"Validation Average Recall:\", avg_recall)\n",
    "print(\"Validation Average Training Time (seconds):\", avg_training_time)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "    # Predict labels for the test set\n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "print(\"Test F1 Score:\", f1_score_test)\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Not Attrition\", \"Attrition\"], \n",
    "            yticklabels=[\"Not Attrition\", \"Attrition\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying Resampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random oversampling\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "x_train_resampled, y_train_resampled = oversampler.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': best_lr,                   # Learning rate (step size shrinkage)\n",
    "    'max_depth': best_max_depth,               # Maximum depth of a tree\n",
    "    'gamma': best_gamma,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'min_child_weight': best_min_child_weight,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "    'num_boost_round': best_num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "    'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "    'lambda': best_lambda,                  # L2 regularization term on weights\n",
    "    'alpha': best_alpha,                   # L1 regularization term on weights\n",
    "    \n",
    "    'eval_metric': best_eval_metric,           # Evaluation metric used during training\n",
    "    'booster': best_booster,          # Type of boosting model\n",
    "\n",
    "    'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "    'objective': best_objective,  # Learning task and corresponding objective function\n",
    "    'verbosity': 0,               # Verbosity of output messages\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Train the model using KFold cross-validation\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "training_times = []\n",
    "\n",
    "for train_index, val_index in k_fold.split(x_train_resampled):\n",
    "    X_train_fold, X_val_fold = x_train_resampled.iloc[train_index], x_train_resampled.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = xgb_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    f1 = f1_score(y_val_fold, y_pred)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Calculate average training time\n",
    "avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "print(\"Validation Average Accuracy:\", avg_accuracy)\n",
    "print(\"Validation Average F1 Score:\", avg_f1_score)\n",
    "print(\"Validation Average Precision:\", avg_precision)\n",
    "print(\"Validation Average Recall:\", avg_recall)\n",
    "print(\"Validation Average Training Time (seconds):\", avg_training_time)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "    # Predict labels for the test set\n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "print(\"Test F1 Score:\", f1_score_test)\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Not Attrition\", \"Attrition\"], \n",
    "            yticklabels=[\"Not Attrition\", \"Attrition\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': best_lr,                   # Learning rate (step size shrinkage)\n",
    "    'max_depth': best_max_depth,               # Maximum depth of a tree\n",
    "    'gamma': best_gamma,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'min_child_weight': best_min_child_weight,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "    'num_boost_round': best_num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "    'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "    'lambda': best_lambda,                  # L2 regularization term on weights\n",
    "    'alpha': best_alpha,                   # L1 regularization term on weights\n",
    "    \n",
    "    'eval_metric': best_eval_metric,           # Evaluation metric used during training\n",
    "    'booster': best_booster,          # Type of boosting model\n",
    "\n",
    "    'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "    'objective': best_objective,  # Learning task and corresponding objective function\n",
    "    'verbosity': 0,               # Verbosity of output messages\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Train the model using KFold cross-validation\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "training_times = []\n",
    "\n",
    "for train_index, val_index in k_fold.split(x_train_resampled):\n",
    "    X_train_fold, X_val_fold = x_train_resampled.iloc[train_index], x_train_resampled.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = xgb_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    f1 = f1_score(y_val_fold, y_pred)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Calculate average training time\n",
    "avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "print(\"Validation Average Accuracy:\", avg_accuracy)\n",
    "print(\"Validation Average F1 Score:\", avg_f1_score)\n",
    "print(\"Validation Average Precision:\", avg_precision)\n",
    "print(\"Validation Average Recall:\", avg_recall)\n",
    "print(\"Validation Average Training Time (seconds):\", avg_training_time)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "    # Predict labels for the test set\n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "print(\"Test F1 Score:\", f1_score_test)\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Not Attrition\", \"Attrition\"], \n",
    "            yticklabels=[\"Not Attrition\", \"Attrition\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "x_train_resampled, y_train_resampled = undersampler.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'eta': best_lr,                   # Learning rate (step size shrinkage)\n",
    "    'max_depth': best_max_depth,               # Maximum depth of a tree\n",
    "    'gamma': best_gamma,                    # Minimum loss reduction required to make a further partition on a leaf node\n",
    "    'min_child_weight': best_min_child_weight,           # Minimum sum of instance weight (hessian) needed in a child'subsample': 1.0,             # Subsample ratio of the training instances\n",
    "    'num_boost_round': best_num_boost_round,       # Number of boosting rounds (trees) to run\n",
    "    'colsample_bytree': 1.0,      # Subsample ratio of columns when constructing each tree\n",
    "    'lambda': best_lambda,                  # L2 regularization term on weights\n",
    "    'alpha': best_alpha,                   # L1 regularization term on weights\n",
    "    \n",
    "    'eval_metric': best_eval_metric,           # Evaluation metric used during training\n",
    "    'booster': best_booster,          # Type of boosting model\n",
    "\n",
    "    'scale_pos_weight': ratio,        # Ratio of negative samples to positive samples\n",
    "    'objective': best_objective,  # Learning task and corresponding objective function\n",
    "    'verbosity': 0,               # Verbosity of output messages\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize CatBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Train the model using KFold cross-validation\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "training_times = []\n",
    "\n",
    "for train_index, val_index in k_fold.split(x_train_resampled):\n",
    "    X_train_fold, X_val_fold = x_train_resampled.iloc[train_index], x_train_resampled.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled.iloc[train_index], y_train_resampled.iloc[val_index]\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model\n",
    "    xgb_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate training time\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = xgb_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "    precision = precision_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    recall = recall_score(y_val_fold, y_pred, zero_division=0)  # Set zero_division parameter here\n",
    "    f1 = f1_score(y_val_fold, y_pred)\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "avg_precision = sum(precisions) / len(precisions)\n",
    "avg_recall = sum(recalls) / len(recalls)\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "# Calculate average training time\n",
    "avg_training_time = sum(training_times) / len(training_times)\n",
    "\n",
    "print(\"Validation Average Accuracy:\", avg_accuracy)\n",
    "print(\"Validation Average F1 Score:\", avg_f1_score)\n",
    "print(\"Validation Average Precision:\", avg_precision)\n",
    "print(\"Validation Average Recall:\", avg_recall)\n",
    "print(\"Validation Average Training Time (seconds):\", avg_training_time)\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "    # Predict labels for the test set\n",
    "y_pred_test = xgb_model.predict(x_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_score_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "print(\"Test F1 Score:\", f1_score_test)\n",
    "print(\"Test Precision:\", precision_test)\n",
    "print(\"Test Recall:\", recall_test)\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Not Attrition\", \"Attrition\"], \n",
    "            yticklabels=[\"Not Attrition\", \"Attrition\"])\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
